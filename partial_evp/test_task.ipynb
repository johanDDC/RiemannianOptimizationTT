{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import ttax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "dot_prod = lambda x: ttax.ops.flat_inner(x, x)\n",
    "full_dot_prod = lambda x: jnp.tensordot(x, x, len(x.shape))\n",
    "norm_TT = lambda x: jnp.sqrt(dot_prod(x))\n",
    "norm_full = lambda x: jnp.sqrt(full_dot_prod(x))\n",
    "\n",
    "actual_grad = lambda x: 2 * x\n",
    "ttax_grad = ttax.autodiff.grad(dot_prod)\n",
    "ttax_full_grad = jax.grad(full_dot_prod)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "failed_tensor = None\n",
    "def test(tensor):\n",
    "    diff = ttax_grad(tensor) + (-1) * actual_grad(tensor)\n",
    "    print(\"dot\", dot_prod(diff))\n",
    "    if dot_prod(diff) < 0:\n",
    "        return test(ttax.orthogonalize(tensor))\n",
    "    return norm_TT(diff) / norm_TT(tensor)\n",
    "\n",
    "def test_full(tensor):\n",
    "    diff = ttax_full_grad(tensor) + (-1) * actual_grad(tensor)\n",
    "    return norm_full(diff) / norm_full(tensor)\n",
    "\n",
    "def test_round(tensor):\n",
    "    diff = ttax.round(ttax.round(ttax_grad(tensor)) +\n",
    "                      ttax.round((-1) * ttax.round(actual_grad(tensor))))\n",
    "    if dot_prod(diff) < 0:\n",
    "        return test_round(ttax.orthogonalize(tensor))\n",
    "    return norm_TT(diff) / norm_TT(tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TT_rank is 5\n",
      "Difference between gradients\n",
      "dot -0.29713368\n",
      "dot 0.0\n",
      "\t\t\tTensor in TT format: 0.0\n",
      "\t\t\tTensor with rounding: 2.5066938e-06\n",
      "TT_rank is 6\n",
      "Difference between gradients\n",
      "dot 191.76631\n",
      "\t\t\tTensor in TT format: 0.0006665873\n",
      "\t\t\tTensor with rounding: 3.4745083e-06\n",
      "TT_rank is 3\n",
      "Difference between gradients\n",
      "dot 0.00069228926\n",
      "\t\t\tTensor in TT format: 0.0010049982\n",
      "\t\t\tTensor with rounding: 1.1359762e-06\n",
      "TT_rank is 3\n",
      "Difference between gradients\n",
      "dot -4.9602706e-05\n",
      "dot -0.0007324219\n",
      "dot -0.00044976198\n",
      "dot 0.00024414062\n",
      "\t\t\tTensor in TT format: 0.00045103356\n",
      "\t\t\tTensor with rounding: 9.137934e-07\n",
      "TT_rank is 5\n",
      "Difference between gradients\n",
      "dot -3.6882758\n",
      "dot -1.0884006\n",
      "dot -2.4634006\n",
      "dot -1.5\n",
      "dot -1.625\n",
      "dot -2.5518012\n",
      "dot -0.125\n",
      "dot 1.0\n",
      "\t\t\tTensor in TT format: 0.00051226997\n",
      "\t\t\tTensor with rounding: 2.5806846e-06\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "n = 5\n",
    "for _ in range(n):\n",
    "    tt_rank = random.randint(3, 7)\n",
    "    modes = random.randint(1, 30, tt_rank - 1)\n",
    "    TT_tensor = ttax.random.tensor(jax.random.PRNGKey(42),\n",
    "                          modes, tt_rank=tt_rank, dtype=jnp.float32)\n",
    "    print(\"TT_rank is\", tt_rank)\n",
    "    print(\"Difference between gradients\")\n",
    "    print(\"\\t\\t\\tTensor in TT format:\", test(TT_tensor))\n",
    "    # print(\"\\t\\t\\tTensor in dense format:\", test_full(ttax.full(TT_tensor)))\n",
    "    print(\"\\t\\t\\tTensor with rounding:\", test_round(TT_tensor))\n",
    "    # print(\"TT_rank is\", tt_rank, \" test with rounding result\", test_round(ttax.full(TT_tensor)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inner product in TTAX: 3.3848824e-05 \tinner product in JAX: 1.4503537e-12\n",
      "inner product in TTAX: 4.2504646e-05 \tinner product in JAX: 4.2257725e-12\n",
      "inner product in TTAX: -2.6522805e-06 \tinner product in JAX: 7.4804746e-14\n",
      "inner product in TTAX: -2.6527265e-05 \tinner product in JAX: 2.299886e-12\n",
      "inner product in TTAX: -2.355309e-05 \tinner product in JAX: 3.644405e-12\n",
      "inner product in TTAX: 0.00071999175 \tinner product in JAX: 2.6259729e-11\n",
      "inner product in TTAX: -2.0049207e-05 \tinner product in JAX: 2.970988e-12\n",
      "inner product in TTAX: 0.08153069 \tinner product in JAX: 1.0617368e-08\n",
      "inner product in TTAX: -0.13295794 \tinner product in JAX: 8.215069e-08\n",
      "inner product in TTAX: -3.791185e-06 \tinner product in JAX: 6.906975e-13\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "for _ in range(n):\n",
    "    tt_rank = random.randint(3, 6)\n",
    "    modes = random.randint(1, 30, tt_rank - 1)\n",
    "    TT_tensor1 = ttax.random.tensor(jax.random.PRNGKey(42),\n",
    "                          modes, tt_rank=tt_rank, dtype=jnp.float32)\n",
    "    TT_tensor2 = ttax.random.tensor(jax.random.PRNGKey(42),\n",
    "                          modes, tt_rank=tt_rank, dtype=jnp.float32)\n",
    "    TT_tensor = TT_tensor2 + (-1) * TT_tensor1\n",
    "    print(\"inner product in TTAX:\", dot_prod(TT_tensor),\n",
    "          \"\\tinner product in JAX:\", full_dot_prod(ttax.full(TT_tensor)))\n",
    "    if failed_tensor is None and dot_prod(TT_tensor) < 0:\n",
    "        failed_tensor = TT_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTAX inner product\t\t\t -2.6522805e-06\n",
      "JAX inner product\t\t\t 7.4804746e-14\n",
      "T3F inner product\t\t\t -2.6522805e-06\n",
      "All tensor elements are squeezed between -0.000000 and 0.000000\n"
     ]
    }
   ],
   "source": [
    "def naive_tensor_inner_product(tensor : ttax.base_class.TT):\n",
    "    res = jnp.einsum('rni,rnj->ij', tensor.tt_cores[0], tensor.tt_cores[0])\n",
    "    for i in range(1, len(tensor.tt_cores)):\n",
    "        cumul = jnp.einsum('anb,cnd->abcd', tensor.tt_cores[i], tensor.tt_cores[i])\n",
    "        res = jnp.einsum('ij,iajb->ab', res, cumul)\n",
    "    return jnp.squeeze(res)\n",
    "\n",
    "def t3f_inner_product(tt_a: ttax.base_class.TT, tt_b : ttax.base_class.TT):\n",
    "  axes_str = 'i'\n",
    "  init_einsum_str = '{1}a{0}b,{2}c{0}d->{3}bd'.format(axes_str, '',\n",
    "                                                      '',\n",
    "                                                      '')\n",
    "  a_core = tt_a.tt_cores[0]\n",
    "  b_core = tt_b.tt_cores[0]\n",
    "  # Simplest example of this operation:\n",
    "  # if both arguments are TT-tensors, then it is\n",
    "  # res = tf.einsum('aib,cid->bd', a_core, b_core)\n",
    "  res = jnp.einsum(init_einsum_str, a_core, b_core)\n",
    "\n",
    "  einsum_str = '{3}ac,{1}a{0}b,{2}c{0}d->{3}bd'.format(axes_str, '',\n",
    "                                                       '',\n",
    "                                                       '')\n",
    "  for core_idx in range(1, tt_a.ndim):\n",
    "    a_core = tt_a.tt_cores[core_idx]\n",
    "    b_core = tt_b.tt_cores[core_idx]\n",
    "    # Simplest example of this operation:\n",
    "    # if both arguments are TT-tensors, then it is\n",
    "    # res = tf.einsum('ac,aib,cid->bd', res, a_core, b_core)\n",
    "    res = jnp.einsum(einsum_str, res, a_core, b_core)\n",
    "  return jnp.squeeze(res)\n",
    "\n",
    "new_failed_tensor = failed_tensor\n",
    "\n",
    "print(\"TTAX inner product\\t\\t\\t\", ttax.ops.flat_inner(new_failed_tensor, new_failed_tensor))\n",
    "print(\"JAX inner product\\t\\t\\t\", jnp.tensordot(ttax.full(new_failed_tensor), ttax.full(new_failed_tensor), len(new_failed_tensor.tt_cores)))\n",
    "print(\"T3F inner product\\t\\t\\t\", t3f_inner_product(new_failed_tensor, new_failed_tensor))\n",
    "print(\"All tensor elements are squeezed between %f and %f\"\n",
    "      %(jnp.amin(ttax.full(new_failed_tensor)), jnp.amax(ttax.full(new_failed_tensor))))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div style=\"\n",
    "        background-color: #FFF3C2;\n",
    "        padding: 6px;\n",
    "        border-radius: 7px;\n",
    "    \">\n",
    "To sum up: there is an awful error in inner product of T3F and TTAX without orthogonalization.\n",
    "JAX seem to do orthogonalization by itself.\n",
    "</div>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}